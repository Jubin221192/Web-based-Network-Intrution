{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz\",\n",
    "                              \"kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./kddcup.data.gz MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is telling us that the underlying file is located in Mark map partitions \n",
    "RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization with Spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_rdd = sc.parallelize(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RDD operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_normal = raw_data.filter(lambda line: \"normal.\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972781"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file = raw_data.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '215',\n",
       "  '45076',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '162',\n",
       "  '4528',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2',\n",
       "  '2',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1',\n",
       "  '1',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '236',\n",
       "  '1228',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '2',\n",
       "  '2',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.50',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '233',\n",
       "  '2032',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2',\n",
       "  '2',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '3',\n",
       "  '3',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.33',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '239',\n",
       "  '486',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '3',\n",
       "  '3',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '4',\n",
       "  '4',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.25',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have calculated time taken by count operation when a 10 percent of sample data is taken and when the count operatoin is applied to entire data set.\n",
    "\n",
    "Moreover, while using small data-set it is easy find any bug.\n",
    "\n",
    "Here we have used a filter it will only look for those line which contains normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = raw_data.sample(False,0.1,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_sample = sample.map(lambda x: x.split(\",\")).filter(lambda x: \"normal\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "num_samp = con_sample.count()\n",
    "duration = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.94949984550476"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_norm = raw_data.map(lambda x: x.split(\",\")).filter(lambda x: \"normal\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "num_samp = contains_norm.count()\n",
    "duration = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.184983015060425"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datasets and Creating New Combinations using Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sample = sample.filter(lambda line: \"normal.\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sample = sample.subtract(normal_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490705"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample contains line having normal in it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97404"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample contains data points which does not cantain normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393301"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = sample.map(lambda line: line.split(\",\")).map(lambda features: features[1]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2= sample.map(lambda line: line.split(\",\")).map(lambda features: features[2]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = feature_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp', 'udp', 'icmp']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = feature_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http',\n",
       " 'finger',\n",
       " 'auth',\n",
       " 'domain_u',\n",
       " 'smtp',\n",
       " 'ftp',\n",
       " 'telnet',\n",
       " 'eco_i',\n",
       " 'ntp_u',\n",
       " 'ecr_i',\n",
       " 'other',\n",
       " 'private',\n",
       " 'pop_3',\n",
       " 'ftp_data',\n",
       " 'daytime',\n",
       " 'remote_job',\n",
       " 'supdup',\n",
       " 'name',\n",
       " 'ssh',\n",
       " 'domain',\n",
       " 'gopher',\n",
       " 'time',\n",
       " 'rje',\n",
       " 'ctf',\n",
       " 'mtp',\n",
       " 'X11',\n",
       " 'urp_i',\n",
       " 'pm_dump',\n",
       " 'IRC',\n",
       " 'exec',\n",
       " 'bgp',\n",
       " 'nnsp',\n",
       " 'iso_tsap',\n",
       " 'http_443',\n",
       " 'login',\n",
       " 'shell',\n",
       " 'printer',\n",
       " 'efs',\n",
       " 'courier',\n",
       " 'uucp',\n",
       " 'kshell',\n",
       " 'klogin',\n",
       " 'whois',\n",
       " 'echo',\n",
       " 'discard',\n",
       " 'systat',\n",
       " 'netstat',\n",
       " 'hostnames',\n",
       " 'csnet_ns',\n",
       " 'pop_2',\n",
       " 'sunrpc',\n",
       " 'uucp_path',\n",
       " 'nntp',\n",
       " 'netbios_ns',\n",
       " 'netbios_ssn',\n",
       " 'netbios_dgm',\n",
       " 'imap4',\n",
       " 'sql_net',\n",
       " 'vmnet',\n",
       " 'link',\n",
       " 'Z39_50',\n",
       " 'ldap',\n",
       " 'urh_i',\n",
       " 'tftp_u',\n",
       " 'red_i',\n",
       " 'tim_i']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cartesian product finding total combination possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_1.cartesian(feature_2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = csv.filter(lambda x: x[41] == \"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are converting the data into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = norm_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = duration.reduce(lambda x, y: x+y)\n",
    "# 21189753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration_count = duration.aggregate((0,0),(lambda db, new_value: (db[0] + new_value, db[1] + 1)),(lambda db1, db2: (db1[0] +db2[0] ,db1[1]+db2[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot tabling with key value paired data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x[41] is the key and x is the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.',\n",
       "  ['0',\n",
       "   'tcp',\n",
       "   'http',\n",
       "   'SF',\n",
       "   '215',\n",
       "   '45076',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '1',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '1',\n",
       "   '1',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '1.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   '0.00',\n",
       "   'normal.'])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv = csv.map(lambda x: (x[41], x))\n",
    "kv.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'normal.': 972781,\n",
       "             'buffer_overflow.': 30,\n",
       "             'loadmodule.': 9,\n",
       "             'perl.': 3,\n",
       "             'neptune.': 1072017,\n",
       "             'smurf.': 2807886,\n",
       "             'guess_passwd.': 53,\n",
       "             'pod.': 264,\n",
       "             'teardrop.': 979,\n",
       "             'portsweep.': 10413,\n",
       "             'ipsweep.': 12481,\n",
       "             'land.': 21,\n",
       "             'ftp_write.': 8,\n",
       "             'back.': 2203,\n",
       "             'imap.': 12,\n",
       "             'satan.': 15892,\n",
       "             'phf.': 4,\n",
       "             'nmap.': 2316,\n",
       "             'multihop.': 7,\n",
       "             'warezmaster.': 20,\n",
       "             'warezclient.': 1020,\n",
       "             'spy.': 2,\n",
       "             'rootkit.': 10})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_duration = csv.map(lambda x: (x[41],float(x[0]))).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 211895753.0),\n",
       " ('buffer_overflow.', 2751.0),\n",
       " ('loadmodule.', 326.0),\n",
       " ('perl.', 124.0),\n",
       " ('neptune.', 2.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 144.0),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 24257982.0),\n",
       " ('ipsweep.', 13049.0),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 259.0),\n",
       " ('back.', 284.0),\n",
       " ('imap.', 72.0),\n",
       " ('satan.', 500.0),\n",
       " ('phf.', 18.0),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 1288.0),\n",
       " ('warezmaster.', 301.0),\n",
       " ('warezclient.', 627563.0),\n",
       " ('spy.', 636.0),\n",
       " ('rootkit.', 1008.0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_duration.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Summary Statistics with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = raw_data.map(lambda x: x.split(\",\"))\n",
    "duration = csv.map(lambda x:[int(x[0])])\n",
    "urgent = csv.map(lambda x:[int(x[8])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_duration = Statistics.colStats(duration)\n",
    "sum_urgent = Statistics.colStats(urgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.34243046395863"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_duration.mean()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.961733052889793e-06"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_urgent.mean()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723.3298112546713"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(sum_duration.variance()[0]) # std. dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007215083685169108"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(sum_urgent.variance()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58329.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pearson to discover correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 4.12205545e-02, 2.03915936e-02],\n",
       "       [4.12205545e-02, 1.00000000e+00, 2.39337570e-04],\n",
       "       [2.03915936e-02, 2.39337570e-04, 1.00000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duration, src_bytes, dst_bytes\n",
    "metric = csv.map(lambda x:[x[0], x[4], x[5]])\n",
    "Statistics.corr(metric, method= \"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SparkSQL for manipulating Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"./kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cont = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = raw_data.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = csv.map(lambda p: Row(duration = int(p[0]), protocol = p[1], service = p[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_cont.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|   10217|\n",
      "|   11610|\n",
      "|   13724|\n",
      "|   10934|\n",
      "|   12026|\n",
      "|    5506|\n",
      "|   12381|\n",
      "|    9733|\n",
      "|   17932|\n",
      "|   40504|\n",
      "|   11565|\n",
      "|   12454|\n",
      "|    9473|\n",
      "|   12865|\n",
      "|   11288|\n",
      "|   10501|\n",
      "|   14479|\n",
      "|   10774|\n",
      "|   10007|\n",
      "|   12828|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_cont.sql(\"\"\"Select duration from rdd where protocol ='tcp' and duration > 2000\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_cont.sql(\"\"\"Select duration from rdd where protocol ='tcp' and service ='http'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(service)|\n",
      "+--------------+\n",
      "|       1870598|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_cont.sql(\"\"\"Select count(service) from rdd where protocol ='tcp'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
